# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-1gEebVNRhb5LevNKLdfwhfBEaZt_iv

# Mental Health EDA

The following notebook reviews the mental health sentiment dataset from Kagle, performing exploratory data analysis on the dataset. The goal is to understand more about the distribution of the data to better inform  what tools can best automate the prediction of these mental health issues.

## Setup the environment
"""

import pandas as pd
from google.colab import drive

drive.mount('/content/data')

# let's download the data
!curl -L -o '/content/data/MyDrive/Colab Notebooks/sentiment-analysis-for-mental-health.zip'  https://www.kaggle.com/api/v1/datasets/download/suchintikasarkar/sentiment-analysis-for-mental-health

# finally let's unzip that zip file
! unzip '/content/data/MyDrive/Colab Notebooks/sentiment-analysis-for-mental-health.zip'

# The file has its own index so we will set the first column to the index value.
df = pd.read_csv('/content/Combined Data.csv', index_col=0)

df.head()

df.shape

df.info()

df.isna().sum()

"""*So, we have 53,403 entries and that number is reflected in the number of status entries. However, we only have 52681 non-null statement entries. Performing an isna illustrates we have 362 statements that are not accounted for.*

## Clean the dataset
"""

df.loc[df['statement'].isna()]

"""*This corroborates what we had calculated. We have 362 statements that are nulls.*"""

# let's clean up the dataset.
cleaned_df = df.loc[df.notna().all(1)]

cleaned_df.shape

cleaned_df.info()

"""## Exploration

### Sentiment Distribution
"""

dist_df = cleaned_df['status'].value_counts().reset_index(name='total')

dist_df.head(10)

dist_df.plot.bar(x='status', y='total')

dist_df.describe()

"""*The dataset is severly imbalanced. with the minimum amount being 1,077 examples for the smallest category and 16,343 examples for the largest. We may need to perform some down sampling and upsampling to even out the distribution.*

*One approach is to make use of the mean or median as the target value to upsample  and or downsample the categorie. Given the large difference between the minimum and maximum value, I recommend using the smaller of the two which is the median 3841, as the targeted number of observations for all the categories to reduce bias in the dataset.*

*This will be addressed in the preprocessing stage.*
"""

# Remove the original dataframe to save save RAM
df = None

"""### Unique words

The following will check for the number of unique words in the dataset. Off, course we are going to want to remove stop words from the vocabulary first. However, this will give us an estimate of how large a sparse matrix we could be creating for our dataset. This may differ once resampling is done to adjust the imbalances in our dataset.
"""

from sklearn.feature_extraction.text import CountVectorizer

statements = cleaned_df['statement'].to_list()

cv_model = CountVectorizer(encoding='utf-8', min_df=1, stop_words='english')

cv_vecs = cv_model.fit_transform(statements)

cv_vecs.shape

"""*Based on the analysis above, there are 58,930 unique words, even with stopwords being applied. This will not affect the encoding however as I have applied sentence encodings, eliminating the need for individual word encodings. Nevertheless, it is important to identify what words dominate the corpus.*"""

word_counts = cv_vecs.toarray().sum(axis=0)  # Sum word occurrences across all documents
word_list = cv_model.get_feature_names_out()

# Create a DataFrame of word frequencies
df_freq = pd.DataFrame({'word': word_list, 'count': word_counts})

# Sort by frequency (descending) and get top 20 words
top_words = df_freq.sort_values(by="count", ascending=False).head(20)

# Display the top words
print(top_words)

top_words.plot.bar(x='word', y='count')

word_freq_dict = {}
for i in range(0, len(word_list)):
  word_freq_dict[word_list[i]] = word_counts[i]

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Generate a wordcloud from the identified frequencies
wordcloud = WordCloud()
wordcloud.generate_from_frequencies(frequencies=word_freq_dict)
plt.figure(figsize=(10,12))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

"""*I am curious as to whether we could perform classification with BERT without having to transform data with SMOTE. BERT is a transformer architecture and capture more semantic meaning in text than other models. The transformer architecture is also at the heart of ChatGPT.*

*There are many variants of BERT but basic BERT only has an input window of 512 tokens. I don't think any of the statements exceed this window size but this is just a small check to see if it does.*
"""

import re

def calculate_token_length(text):
  '''
    Calculate the length of tokens generated from the text string
  '''
  #print(text)
  pattern = re.compile(r'\b\w\w+\b', re.UNICODE)
  tokens  = pattern.findall(text)
  #print(tokens)
  return len(tokens)

#calculate_token_length('oh my gosh')

cleaned_df['token_length'] = cleaned_df['statement'].apply(calculate_token_length)
cleaned_df.head()

# Let's order this list by token length to find the string with the longest length
cleaned_df.sort_values(by='token_length').tail()

"""*According to the token length check, there are some values that do generate text with more than 512 tokens. If we do use BERT or similar transformer model, we are going to need a larger one that has the capacity for at least 5000 tokens.*"""

# Removing variables that are memory intensive.
cv_vecs = None
df_freq = None
dist_df = None
statements = None
word_counts = None
top_words = None
word_list = None

cleaned_df = cleaned_df.drop(['token_length'], axis=1)

cleaned_df.head()

"""## Preprocessing

### Resampling dataset

Here, I will attempt to resample the dataset to make it more balanced. The target number of samples for each sentiment category will be 3841, the same as the Anxiety category as shown above.

The techniques are taken from https://wellsr.com/python/upsampling-and-downsampling-imbalanced-data-in-python/ and https://www.geeksforgeeks.org/smote-for-imbalanced-classification-with-python/
"""

from sklearn.utils import resample

"""#### Downsample Cateogories"""

# The category containing the target number of samples
anxiety = cleaned_df.loc[cleaned_df['status'] == 'Anxiety']

# The categories with samples above the target
normal = cleaned_df.loc[cleaned_df['status'] == 'Normal']
depression = cleaned_df.loc[cleaned_df['status'] == 'Depression']
suicidal = cleaned_df.loc[cleaned_df['status'] == 'Suicidal']

print('The number of records for anxiety ', anxiety.shape[0])
print('The number of records for normal ', normal.shape[0])
print('The number of records for depression ', depression.shape[0])
print('The number of records for suicidal ', suicidal.shape[0])

# let's resample those  results
new_normal = resample(normal, replace=True, n_samples=anxiety.shape[0], random_state=1337)
new_depression = resample(depression, replace=True, n_samples=anxiety.shape[0], random_state=1337)
new_suicidal = resample(suicidal, replace=True, n_samples=anxiety.shape[0], random_state=1337)

# the new downsampled categories.
print(new_normal.shape)
print(new_depression.shape)
print(new_suicidal.shape)

"""#### Upsample Categories"""

# let's rebuild the dataset using the values we have now.
bipolar = cleaned_df.loc[cleaned_df['status'] == 'Bipolar']
stress = cleaned_df.loc[cleaned_df['status'] == 'Stress']
personality =  cleaned_df.loc[cleaned_df['status'] == 'Personality disorder']

resampled_df = pd.concat([anxiety, new_normal, new_suicidal, new_depression,
                         bipolar, stress, personality])

# Release memordy for unused variables.
anxiety = None
new_normal = None
new_depression = None
new_suicidal = None
bipolar = None
stress = None
personality = None
suicidal = None
normal = None
depression = None

# Checking the new dataset dimensions before upsampling
resampled_df.shape

resampled_df['status'].value_counts()

! pip install gensim

! pip install fasttext

from imblearn.over_sampling import SMOTE
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import LabelEncoder
import nltk
import numpy as np
import fasttext

nltk.download('punkt_tab')

# Encode the labels into numerical equivalents. This is requried for SMOTE.
le = LabelEncoder()
y = le.fit_transform(resampled_df['status'])

# Tokenize the x values
X_tokens = resampled_df['statement'].apply(lambda x: word_tokenize(x.lower()))

# We are going to make use of the common crawl pre-trained model for english words
! wget 'https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz'

! gunzip 'cc.en.300.bin.gz'

# Load the fasttext model
fasttext_model = fasttext.load_model('cc.en.300.bin')

# Get a list of stopwords to extract from the corpous
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
stopwords_list = list(ENGLISH_STOP_WORDS)

def sentence_vector(tokens, model):
  '''
    Generate a vector/embedding to represent the entire senntence
  '''
  vectors = [model[word] for word in tokens if word not in stopwords_list and word in model]
  return np.mean(vectors, axis=0) if vectors else np.zeros(model.get_dimension())

# Vectorize all the data statements
X = np.array(X_tokens.apply(lambda x: sentence_vector(x, fasttext_model))).tolist()

# Create the smote model
smote_model = SMOTE(random_state=42)

# Use smote to upsample the values in all the categories.
X_resampled, y_resampled = smote_model.fit_resample(X, y)

# Reconstruct the dataframe
rebalanced_df = pd.DataFrame(X_resampled)
rebalanced_df['status'] = y_resampled

rebalanced_df.head()

"""*The new, rebalanced dataset has 300 features, all float, and one integer column label. Each row represents a sentence. Each sentence is represented by a vector of 300 dimensions. The label and the embeddings which generated this dataset will be saved for future training, testing and or encoding as a csv.*

*The status column has numbers from 0-6. Each represent one of the previouly identified mental health statuses. The statuses map to the numbers in alphabetic order.*
"""

# let's save the new rebalanced dataset
rebalanced_df.to_csv('/content/data/MyDrive/Colab Notebooks/mental-health-data-v2.csv')

# Check to see if the different categories are balanced.
dist_df = rebalanced_df['status'].value_counts().reset_index(name='total')
dist_df.plot.bar(x='status', y='total')

status_names = resampled_df['status'].value_counts().index.to_list()
status_keys = rebalanced_df['status'].value_counts().index.to_list()

status_legend = pd.DataFrame(zip(status_names, status_keys), columns=['status_names', 'status_keys'])
status_legend.head(10)

"""*The rebalanced dataset status values are the text status values but listed in alphabetical order. Consequently Anxiety = 0, Bipolar = 1 etc. A complete legend is shown above*"""

# Removing temporary variables to clear space
y = None
X = None
X_tokens = None
X_resampled = None
y_resampled = None

resampled_df = None
rebalanced_df = None
cleaned_df = None

"""## Next steps

Now that we have a clean dataset, our next steps would be to pick an ideal model that can train and help predict new outcomes. It is not clear from the EDA performed whether whether the data is linearly seperable or not. Consequently, we may want to try a combination of linear and non-linear models to see which best fits the goal.

We can make use of traditional machine learning models, neural networks or even transformers. However, if we make use of a transformer, we may not be able to take advantage of the newly generated rebalanced, and resampled datasaset.

Transformers are designed to work primarly on text. However, in order to upsample the data, we had to convert the data to numeric values. This would be difficult to predict using a transformer. Lastly, if we made use of a transformer like BERT which can only handle iput of 512 tokens by default, we may miss out on text which includes longer token sequences.

"""

