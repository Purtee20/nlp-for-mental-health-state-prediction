# -*- coding: utf-8 -*-
"""Model_Comparison.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MEFvOOC4km1CULZ_aDiDkTdeNFs6VZL5

# Mental Health Model Comparison


The purpose of this notebook is to continue the work started in the EDA. The EDA process was used to generate a balanced dataset which could be further analyzed. This notebook is meant to identify the best model for the dataset. I will be looking at some of the best models that can be used for both linear and non-linear datasets. At this point, it is not clear whether the data is linear or note. It is hoped the model will help to identify which category the dataset falls into and at the same time select a model which is representative of the data.

Some of the models I will be looking at are LogisticRegression, Linear SVM, Random Forest, and an ANN.

## Prepare data
"""

import pandas as pd
from google.colab import drive

# Set the random state
r_state = 42

drive.mount('/content/data')

# The first column has been set as the index column
df = pd.read_csv('/content/data/MyDrive/Colab Notebooks/mental-health-data-v2.csv', index_col=0)

df.head()

df.info()

# let's split the data
from sklearn.model_selection import train_test_split

X = df.drop(['status'], axis=1)
y = df['status']

disorder_labels = ['anxiety', 'bipolar', 'depression', 'normal','personality_disorder', 'stress', 'suicidal']

X_temp,X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state= r_state, stratify=y)
X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=r_state, stratify=y_temp)

print('The shape of X_train: ', X_train.shape)
print('The shape of y_train: ', y_train.shape)

X_train.head()

y_train.head()

"""## Determine Baseline accuracy"""

from sklearn.metrics import classification_report
from sklearn.dummy import DummyClassifier

# let's calculate the baseline accuracy
dummy_class = DummyClassifier(strategy='stratified', random_state=r_state)
dummy_model = dummy_class.fit(X_train, y_train)

y_dum_val = dummy_model.predict(X_val)

result = classification_report(y_val, y_dum_val, target_names=disorder_labels)
print(result)

"""*This was unexpected but the dummy classification was a lot lower than expected. The accuracy was only 14%. This meamans it did very poorly at classifying the dataset. The scores for the precision, recall and f1-scores are also bad. The __normal__ class however appears to be consisistently better than all the other classes, scoring the highest of the scores across all metrics. This reflects the findings of the EDA of the dataset which indicated the data was imbalanced and that the __normal__ class had more representation than all other classes.*

## Linear SVC
"""

from sklearn.svm import LinearSVC

# let's train the model
svc_class = LinearSVC(max_iter=2000, random_state=r_state)
svc_model = svc_class.fit(X_train, y_train)

# let's predict with validation test data
y_svc_val = svc_model.predict(X_val)

result = classification_report(y_val, y_svc_val, target_names=disorder_labels)
print(result)

"""*The linear SVC did pretty well, scoring way above the baseline accuracy of 14%.It scored 67%. The other metric scores were also above the baseline. I should point out that for the f1-score, __anxiety__ scored higher than __normal__ class. __Anxiety__ was chosen as the target class for its number of representations. It provided a good middle ground for the number of samples to appear for all classes.*

*The classes with the least amount of samples, i.e. lower than the target number of samples - __bipolar, stress and personality_disorder__ all scored surprisingly well for the f1-score. The scored 73%, 61% and 73% respectively*

*The most surprising for me was __depression__ which scored least of all the classes even though it had more samples than the target samples and thus should have had more examples to choose from. It only scored 47%. There may be other nuances in the language at play resulting in the difficulty in getting a higher f1-score.*

## Logisitic Regression
"""

from sklearn.linear_model import LogisticRegression

logr_class = LogisticRegression(max_iter=2000, random_state=r_state)
logr_model = logr_class.fit(X_train, y_train)

y_logr_val = logr_class.predict(X_val)

result = classification_report(y_val, y_logr_val, target_names=disorder_labels)
print(result)

"""*The logistic regression did not score as high as I expected. It scored less than the LinearSVC. It scored an overall accuracy of 63%. However, it was still higher than the baseline accuracy.*

*Although it scored lower than the LinearSVC, the characteristics in the results are similar to that of the LinearSVC. The highest class in the fl-score is __anxiety__. The lowest classs is __drepression__. __Bipolar, stress, and personality_disorder__ performed suprisingly well and were all above baseline. The overall behavior is similar with only the scores being different.*

## Random Forest
"""

from  sklearn.ensemble import RandomForestClassifier

tree_class = RandomForestClassifier(random_state=r_state)
tree_model = tree_class.fit(X_train, y_train)

y_tree_val = tree_model.predict(X_val)

result = classification_report(y_val, y_tree_val, target_names=disorder_labels)
print(result)

"""*Configuring random forest can be an art in itself, especially when it comes to determining the maximum depth of the tree. If you choose too little depth, it affects the accuracy. If you choose too much you risk overfitting. In this case, I chose to let the algorithm decide on the depth.*

*With the default settings, the model achieves an accuracy of 73%. That is very good. However, in this model, __personality_disorder__ has the highest f1-score. The __personality_disorder__ class scored the higest across all metrics, scoring 86%, 91% and 89% respectively. This might not be a good thing as __personality_disorder__ rerpesented the smallest class and its values were upsampled. Consequently, there may be some overfitting going on.*

*__Normal__ is the second highest which is expected since it had the highest number of samples among the classes and had to be down sampled. Surprisingly, __depression__ is still the lowest in the f1-score. I would have thought this time around it would have scored higher but I am becoming more confident that it is inherent in nuances in the language.*

## Artificial Neural Network (ANN)
"""

from keras import Sequential
from keras.layers import Input, Dense
from keras.utils import to_categorical
from keras.models import Model
import numpy as np

# convert y_train to a sparse vector
y_train_cat = to_categorical(y_train, num_classes=7)

"""### Model 1

This model adds 150 neurons to the 300 dimension input for better representation.
"""

# Adding 150 neurons to the hidden layer

model1 = Sequential(layers=[
    Input(shape=(300, ), name='input_layer'),
    Dense(450, activation='relu',  name='model2_d1'),
    Dense(7, activation='softmax', name='output_layer')
])

model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model1.fit(X_train, y_train_cat, epochs=20)

y_model1_val = model1.predict(X_val)

y_model1_val1 = np.argmax(y_model1_val, axis=1)

result = classification_report(y_val, y_model1_val1, target_names=disorder_labels)
print(result)

"""*Possible overfitting. Training accuracy got to 81.3% but validation accuracy was only up to 75%*

### Model 2

This model creates two hidden layers. One with 450 neurons and another with 300 neurons.
"""

model2 = Sequential(layers=[
    Input(shape=(300, ), name='input_layer'),
    Dense(450, activation='relu',  name='model2_d1'),
    Dense(300, activation='relu', name='model2_d2'),
    Dense(7, activation='softmax', name='output_layer')
])

model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model2.fit(X_train, y_train_cat, epochs=20)

y_model2_val = model2.predict(X_val)

y_model2_val2 = np.argmax(y_model2_val, axis=1)

result = classification_report(y_val, y_model2_val2, target_names=disorder_labels)
print(result)

"""*Another possible occurrence of overfitting. The model scored 88.9% on training but 76% on validation.*

### Model 3

There were significant amounts of overfitting recognized in the last model and a little noticed in model1. Perhaps incrasing the number of neurons is not the best way to go. I will attempt to reduce the number of neurons.
"""

# Reducing neurons by half

model3 = Sequential(layers=[
    Input(shape=(300, ), name='input_layer'),
    Dense(150,  activation='relu', name='model_3_d1'),
    Dense(7, activation='softmax', name='output_layer')
])

model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model3.fit(X_train, y_train_cat, epochs=20)

y_model3_val = model3.predict(X_val)

y_model3_val3 = np.argmax(y_model3_val, axis=1)

result = classification_report(y_val, y_model3_val3, target_names=disorder_labels)
print(result)

"""*By halving the number of neurons, the representational power of the netowrk was reduced. Model 1 had an accuracy of 75% but Model 3 which has half the number of neurons scored 72%.*

*Looking back at the training scores, the training accuracy was always higher than the predicted accuracy on the validation data. Consequently, there is some overfitting going on. It may be possible to get even similar scores on a smaller set of neurons because of this overfitting. I am going to attempt Dropout on the Model 3 configuration.*

### Model 3 with Dropout
"""

from keras.layers import Dropout

model3_wd = Sequential(layers=[
    Input(shape=(300, ), name='input_layer'),
    Dense(150, activation='relu',  name='model2_d1'),
    Dropout(0.2, seed=r_state),
    Dense(7, activation='softmax', name='output_layer')
])

model3_wd.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model3_wd.fit(X_train, y_train_cat, epochs=20)

y_model3_wd_val = model3_wd.predict(X_val)

y_model3_wd_val3 = np.argmax(y_model3_wd_val, axis=1)

result = classification_report(y_val, y_model3_wd_val3, target_names=disorder_labels)
print(result)

"""*Dropout helps reduce the overfitting but there is still some detected. It may require some tweaking to get the right number for dropout and or neurons.*

## Assessment

*Among all the model configurations, it appears that RandomForest does a better job at predicting the mental health sentiment. It produces the same accuracy as an ANN with dropout without the complexity of an ANN. It appears the ensemble method works the best with this dataset. I plan to perform one more test.*

*Usually, Gradient Boosting, which is another ensemble method, gives better results than RandomForest. I am going to perform the same test with gradient boosting.*

|Model|Accuracy|
|-----|--------|
|LinearSVC| 67%|
|LogisticRegression| 63%|
|RandomForest| 73%|
|ANN Model 1| 75%|
|ANN Model 2| 76%|
|ANN Model 3| 72%|
|ANN Model 3 with Dropout| 73%|

## Gradient Boosting
"""

from sklearn.ensemble import HistGradientBoostingClassifier

gb_class = HistGradientBoostingClassifier(random_state=r_state)
gb_model = gb_class.fit(X_train, y_train)

y_gb_val = gb_model.predict(X_val)

result = classification_report(y_val, y_gb_val, target_names=disorder_labels)
print(result)

"""*As anticipated, the GradientBoostClassifier did a better job of classifiying the dataset with an accuracy of 76% compared to the 73% of the RandomForest. I still have test data from the train test split earlier. I am going to test out that data on the model*"""

y_gb_preds = gb_model.predict(X_test)

result = classification_report(y_test, y_gb_preds, target_names=disorder_labels)
print(result)

"""The model scored a slightly higher prediction on new data and scored 77%.

## XGBoost
"""

import xgboost as xgb

# Taken from https://www.kaggle.com/code/stuarthallows/using-xgboost-with-scikit-learn#Multiclass-classification
xgb_model = xgb.XGBClassifier(random_state=r_state, objective="multi:softprob")

xgb_model.fit(X_train, y_train)

y_xgb_val = xgb_model.predict(X_val)

result = classification_report(y_val, y_xgb_val, target_names=disorder_labels)
print(result)

"""The results of using XGboost appear about the same as that as Gradient boosting training with an increase of only 1%. Let me try with raw test data."""

y_xgb_preds = xgb_model.predict(X_test)

result = classification_report(y_test, y_xgb_preds, target_names=disorder_labels)
print(result)

"""Both XGBoost and GradientBoosting appear to be on par with each other with just a marginal increase in performance. I will save the gradient boost model instead though.

## Saving the model
"""

import joblib

filename='/content/data/MyDrive/Colab Notebooks/model.joblib'
joblib.dump(gb_model, filename=filename)